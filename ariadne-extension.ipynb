{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests before extension install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer = 23\n",
    "integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'works fine'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer = \"works fine\"\n",
    "integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and load extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/alanbraz/ipython-ariadne-extension.git\n",
      "  Cloning https://github.com/alanbraz/ipython-ariadne-extension.git to /gpfs/global_fs01/sym_shared/YPProdSpark/user/saa6-1973990e46b9ea-cc8e51e960b7/notebook/tmp/pip-_uyhdduo-build\n",
      "Installing collected packages: ipython-ariadne-extension\n",
      "  Running setup.py install for ipython-ariadne-extension ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed ipython-ariadne-extension-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -I git+https://github.com/alanbraz/ipython-ariadne-extension.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ariadne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adriane DEBUG: check OK!\n"
     ]
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1675d2dfb7d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"123\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adriane DEBUG: check OK!\n"
     ]
    }
   ],
   "source": [
    "1+\"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /gpfs/fs01/user/saa6-1973990e46b9ea-cc8e51e960b7/notebook/tmp/tmpyfwgxdq5\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_save_checkpoints_secs': 600, '_tf_random_seed': 1, '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/gpfs/fs01/user/saa6-1973990e46b9ea-cc8e51e960b7/notebook/tmp/tmpyfwgxdq5', '_keep_checkpoint_max': 5}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f9d2a0a0aebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Define the input function for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m input_fn = tf.estimator.inputs.numpy_input_fn(\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     batch_size=batch_size, num_epochs=None, shuffle=True)\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'train'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adriane diagnostic error: Bad type to convolve pixel[n][28 * 28], needs 4 dimensions (possible fix: tf.reshape(x, [-1, 28, 28, 1]))\n",
      "Line 46:         bad_conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
      "                                              ^\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = []\n",
    "try:\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "except:\n",
    "    exit\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        bad_x = tf.reshape(x, shape=[-1, 11, 28, 1])\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        z = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(z, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        bad_conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Build the neural network\n",
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False,\n",
    "                            is_training=True)\n",
    "    logits_test = make_net(features, num_classes, dropout, reuse=True,\n",
    "                           is_training=False)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op,\n",
    "                                  global_step=tf.train.get_global_step())\n",
    "\n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss_op,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs\n",
    "\n",
    "make_net = conv_net\n",
    "\n",
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)\n",
    "\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Foo'>\n",
      "<function Foo.foo at 0x7fb8a6b907b8>\n",
      "10\n",
      "<function foo at 0x7fb8a6b906a8>\n",
      "3\n",
      "15\n",
      "15\n",
      "<bound method Foo.foo of <__main__.Foo object at 0x7fb8a6b9b9e8>>\n",
      "17\n",
      "9\n",
      "<function foo at 0x7fb8a6b906a8>\n",
      "11\n",
      "<function foo at 0x7fb8a6b906a8>\n",
      "13\n",
      "<function foo at 0x7fb8a6b906a8>\n",
      "<class '__main__.Foo'>\n",
      "<__main__.Foo object at 0x7fb8a6b9bb00>\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adriane DEBUG: check OK but with warnings!\n"
     ]
    }
   ],
   "source": [
    "base_init = 10\n",
    "\n",
    "def id(x):\n",
    "    return x\n",
    "\n",
    "def call(x, y):\n",
    "    return x(y)\n",
    "\n",
    "def foo(a,b):\n",
    "    return call(id, a+b)\n",
    "\n",
    "class Foo(object):\n",
    "    base = base_init\n",
    "    \n",
    "    def foo(self, a, b):\n",
    "        self.contents = id(a+b+self.base)\n",
    "        return self.contents\n",
    "\n",
    "print(Foo)\n",
    "\n",
    "print(Foo.foo)\n",
    "print(Foo.base)\n",
    "\n",
    "print(foo)\n",
    "print(foo(1,2))\n",
    "\n",
    "instance = Foo()\n",
    "print(Foo.foo(instance, 2,3))\n",
    "print(instance.foo(2,3))\n",
    "\n",
    "f = instance.foo\n",
    "print(f);\n",
    "print(f(3,4))\n",
    "\n",
    "instance.f = foo\n",
    "print(instance.f(4,5))\n",
    "print(instance.f);\n",
    "\n",
    "instance.foo = foo;\n",
    "print(instance.foo(5,6))\n",
    "print(instance.foo);\n",
    "\n",
    "foo.x = foo;\n",
    "print(foo.x(6,7));\n",
    "print(foo.x);\n",
    "\n",
    "x = Foo\n",
    "print(x)\n",
    "y = x()\n",
    "print(y)\n",
    "print(y.foo(7,8))\n",
    "\n",
    "def nothing():\n",
    "    return 0\n",
    "\n",
    "z = id(nothing)\n",
    "z()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type annotation of function f: {'return': <class 'int'>, 'x': <class 'int'>}\n",
      "result of calling f('ab'): ab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adriane ERROR calling IBM Function: <class 'urllib.error.HTTPError'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/fs01/user/saa6-1973990e46b9ea-cc8e51e960b7/.local/lib/python3.5/site-packages/ariadne/ariadne.py\", line 44, in check\n",
      "    reply = urllib.request.urlopen(req, jsondataasbytes).read()\n",
      "  File \"/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/urllib/request.py\", line 163, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/urllib/request.py\", line 472, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/urllib/request.py\", line 582, in http_response\n",
      "    'http', request, response, code, msg, hdrs)\n",
      "  File \"/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/urllib/request.py\", line 510, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/urllib/request.py\", line 444, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/urllib/request.py\", line 590, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 502: Bad Gateway\n",
      "\n",
      "jsondata: \"{\\\"code\\\": \\\"import os\\\\nos.environ['MYPYPATH'] = os.environ['PYTHONPATH']\\\\n1+1\\\\n1+\\\\\\\"123\\\\\\\"\\\\nbase_init = 10\\\\n\\\\ndef id(x):\\\\n    return x\\\\n\\\\ndef call(x, y):\\\\n    return x(y)\\\\n\\\\ndef foo(a,b):\\\\n    return call(id, a+b)\\\\n\\\\nclass Foo(object):\\\\n    base = base_init\\\\n    \\\\n    def foo(self, a, b):\\\\n        self.contents = id(a+b+self.base)\\\\n        return self.contents\\\\n\\\\nprint(Foo)\\\\n\\\\nprint(Foo.foo)\\\\nprint(Foo.base)\\\\n\\\\nprint(foo)\\\\nprint(foo(1,2))\\\\n\\\\ninstance = Foo()\\\\nprint(Foo.foo(instance, 2,3))\\\\nprint(instance.foo(2,3))\\\\n\\\\nf = instance.foo\\\\nprint(f);\\\\nprint(f(3,4))\\\\n\\\\ninstance.f = foo\\\\nprint(instance.f(4,5))\\\\nprint(instance.f);\\\\n\\\\ninstance.foo = foo;\\\\nprint(instance.foo(5,6))\\\\nprint(instance.foo);\\\\n\\\\nfoo.x = foo;\\\\nprint(foo.x(6,7));\\\\nprint(foo.x);\\\\n\\\\nx = Foo\\\\nprint(x)\\\\ny = x()\\\\nprint(y)\\\\nprint(y.foo(7,8))\\\\n\\\\ndef nothing():\\\\n    return 0\\\\n\\\\nz = id(nothing)\\\\nz()\\\\ndef f(x: int) -> int:\\\\n     return x\\\\nprint(\\\\\\\"type annotation of function f: \\\\\\\" + str(f.__annotations__))\\\\nprint(\\\\\\\"result of calling f('ab'): \\\\\\\" + str(f('ab')))\\\\n\\\\n# alanbraz: something is breaking the HTTP body post, maybe the arrown?!!?\\\"}\"\n"
     ]
    }
   ],
   "source": [
    "def f(x: int) -> int:\n",
    "     return x\n",
    "print(\"type annotation of function f: \" + str(f.__annotations__))\n",
    "print(\"result of calling f('ab'): \" + str(f('ab')))\n",
    "\n",
    "# alanbraz: something is breaking the HTTP body post, maybe the arrown?!!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
